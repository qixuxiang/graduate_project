import numpy as np
 
# 双曲正切函数,该函数为奇函数
def tanh(x):    
     return np.tanh(x)
 
 # tanh导函数性质:f'(t) = 1- f(x)^2
def tanh_prime(x):      
    return 1.0 - tanh(x)**2


class NeuralNetwork:
    def __init__(self, layers, activation = 'tanh'):
        """
        :参数layers: 神经网络的结构(输入层-隐含层-输出层包含的结点数列表)
        :参数activation: 激活函数类型
        """
        if activation == 'tanh':    # 也可以用其它的激活函数
            self.activation = tanh
            self.activation_prime = tanh_prime
        else:
            pass

        # 存储权值矩阵
        self.weights = []

        # range of weight values (-1,1)
        # 初始化输入层和隐含层之间的权值
        for i in range(1, len(layers) - 1):
            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1 # add 1 for bias node
            self.weights.append(r)
            
        # 初始化输出层权值
        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1       
        self.weights.append(r)


    def fit(self, X, Y, learning_rate=0.20, epochs=10000):
        # Add column of ones to X
        # This is to add the bias unit to the input layer
        X = np.hstack([np.ones((X.shape[0],1)),X])
     
        
        for k in range(epochs):     # 训练固定次数
            if k %1000 == 0: 
            	print('epochs:', k)

            # Return random integers from the discrete uniform distribution in the interval [0, low).
            i = np.random.randint(X.shape[0],high=None) 
            a = [X[i]]   # 从m个输入样本中随机选一组
            for l in range(len(self.weights)): 
                    dot_value =np.dot(a[l], self.weights[l])   # 权值矩阵中每一列代表该层中的一个结点与上一层所有结点之间的权值
                    activation = self.activation(dot_value)
                    a.append(activation)
                    
            # 反向递推计算delta:从输出层开始,先算出该层的delta,再向前计算
            error = Y[i] - a[-1]    # 计算输出层delta
            deltas = [error * self.activation_prime(a[-1])]
            
            # 从倒数第2层开始反向计算delta
            for l in range(len(a) - 2, 0, -1): 
                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))


            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]
            deltas.reverse()    # 逆转列表中的元素


            # backpropagation
            # 1. Multiply its output delta and input activation to get the gradient of the weight.
            # 2. Subtract a ratio (percentage) of the gradient from the weight.
            for i in range(len(self.weights)):  # 逐层调整权值
                layer = np.atleast_2d(a[i])     # View inputs as arrays with at least two dimensions
                delta = np.atleast_2d(deltas[i])
                self.weights[i] += learning_rate * np.dot(layer.T, delta) # 每输入一次样本,就更新一次权值

    def predict(self, x): 
        a = np.concatenate((np.ones(1), np.array(x)))       # a为输入向量(行向量)
        for l in range(0, len(self.weights)):               # 逐层计算输出
            a = self.activation(np.dot(a, self.weights[l]))
        return a



if __name__ =='__main__':
    nn = NeuralNetwork([24,1,48])     # 网络结构: 24输入48输出,1个隐含层(包含2个结点)

    
    X = np.array([[8, 8, 11, 11, 11, 11, 12, 12, 13, 32, 32, 32, 32, 32, 32, 6, 5, 4, 4, 4, 4, 4, 4, 4],
            [12, 11, 13, 13, 13, 14, 12, 13, 12, 12, 13, 12, 13, 12, 13, 13, 13, 14, 16, 15, 15, 12, 11, 9],
            [6, 6, 8, 12, 14, 13, 14, 14, 13, 13, 13, 12, 11, 16, 16, 17, 19, 19, 25, 22, 22, 18, 16, 15],
            [4, 6, 7, 9, 10, 14, 14, 14, 10, 11, 12, 11, 12, 31, 31, 29, 29, 32, 32, 4, 4, 4, 4, 4],
            [3, 20, 21, 23, 23, 22, 21, 15, 14, 13, 12, 12, 12, 13, 14, 18, 16, 18, 22, 21, 19, 17, 16, 13],
            [14, 15, 17, 24, 26, 18, 18, 17, 15, 14, 13, 13, 13, 14, 15, 16, 18, 20, 22, 21, 18, 12, 10, 9],
            [4, 4, 4, 7, 9, 10, 13, 13, 13, 14, 14, 14, 11, 10, 11, 7, 8, 9, 12, 12, 11, 8, 7, 6],
            [5, 8, 9, 19, 22, 25, 27, 26, 18, 14, 15, 13, 15, 13, 14, 18, 24, 25, 24, 22, 13, 8, 7, 6],
            [6, 9, 10, 17, 19, 18, 17, 16, 12, 11, 12, 11, 11, 12, 11, 12, 13, 15, 15, 19, 21, 15, 14, 6],
            [12, 14, 20, 23, 13, 12, 12, 14, 12, 10, 8, 8, 8, 8, 10, 13, 13, 12, 12, 13, 22, 20, 18, 14],
            [8, 9, 10, 21, 23, 24, 20, 19, 18, 20, 21, 21, 18, 17, 14, 29, 30, 31, 29, 29, 28, 7, 6, 6],
            [32, 32, 32, 32, 32, 31, 12, 10, 9, 9, 10, 9, 8, 11, 12, 15, 16, 16, 21, 20, 19, 17, 15, 15],
            [9, 17, 19, 21, 21, 13, 12, 14, 13, 14, 13, 9, 8, 8, 9, 8, 8, 9, 9, 8, 9, 7, 7, 7],
            [15, 15, 17, 19, 20, 21, 14, 14, 14, 12, 11, 9, 9, 9, 9, 8, 8, 9, 31, 32, 32, 32, 32, 31],
            [18, 18, 22, 24, 25, 27, 28, 25, 24, 23, 19, 18, 18, 18, 19, 18, 18, 22, 22, 20, 18, 19, 17, 17],
            [4, 5, 5, 6, 28, 28, 29, 29, 31, 31, 31, 31, 12, 10, 9, 9, 9, 9, 8, 7, 4, 4, 3, 3],
            [13, 14, 14, 21, 24, 23, 19, 19, 16, 13, 13, 12, 9, 14, 13, 14, 17, 17, 28, 29, 28, 28, 28, 26],
            [31, 32, 32, 32, 32, 31, 4, 3, 2, 2, 3, 3, 4, 5, 5, 4, 4, 4, 23, 24, 24, 23, 23, 22],
            [27, 27, 27, 27, 27, 28, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 30, 30, 30, 30, 30],
            [3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 6, 5, 5, 23, 24, 29, 29, 29, 28, 24, 24],
            [30, 32, 32, 32, 30, 3, 2, 2, 6, 7, 8, 8, 9, 8, 8, 9, 9, 6, 5, 6, 6, 4, 3, 2],
            [18, 19, 23, 28, 28, 30, 31, 31, 28, 25, 23, 5, 5, 5, 5, 5, 6, 6, 6, 5, 4, 4, 3, 3],
            [32, 32, 31, 10, 10, 14, 14, 13, 12, 11, 5, 12, 14, 16, 14, 15, 10, 10, 32, 32, 32, 28, 29, 28],
            [29, 32, 32, 32, 32, 30, 4, 4, 5, 5, 5, 4, 6, 6, 6, 7, 7, 7, 31, 31, 31, 29, 29, 27],
            [10, 21, 22, 26, 27, 24, 20, 20, 18, 12, 12, 12, 12, 14, 14, 18, 20, 20, 25, 26, 25, 20, 20, 15],
            [29, 30, 31, 31, 31, 29, 8, 8, 7, 8, 9, 8, 8, 9, 8, 10, 12, 12, 14, 14, 14, 10, 9, 9],
            [12, 13, 15, 17, 17, 17, 13, 12, 12, 10, 9, 8, 8, 9, 9, 8, 9, 11, 31, 32, 32, 32, 32, 29],
            [24, 24, 30, 31, 31, 31, 31, 28, 28, 24, 6, 6, 6, 6, 5, 4, 5, 5, 6, 6, 6, 6, 5, 5],
            [9, 9, 14, 15, 16, 19, 19, 17, 16, 16, 16, 16, 15, 16, 17, 17, 18, 18, 19, 16, 13, 14, 14, 14],
            [5, 5, 6, 7, 8, 28, 29, 29, 30, 30, 32, 32, 32, 32, 13, 12, 12, 11, 11, 12, 12, 11, 10, 10],
            [25, 27, 28, 29, 29, 8, 7, 6, 4, 4, 4, 4, 4, 4, 5, 5, 6, 7, 8, 29, 28, 27, 26, 24],
            [6, 7, 7, 17, 16, 19, 19, 20, 17, 16, 15, 7, 7, 15, 16, 17, 20, 19, 19, 16, 17, 9, 8, 6],
            [7, 10, 24, 23, 22, 20, 7, 20, 22, 21, 16, 6, 6, 21, 22, 22, 16, 7, 18, 22, 25, 25, 11, 7],
            [6, 9, 10, 14, 17, 20, 20, 23, 23, 16, 12, 12, 12, 13, 14, 20, 22, 20, 18, 18, 16, 8, 8, 7],
            [4, 5, 5, 11, 11, 15, 15, 16, 21, 20, 20, 10, 9, 11, 11, 12, 12, 11, 11, 11, 11, 5, 5, 3],
            [12, 15, 16, 17, 17, 19, 19, 16, 16, 17, 17, 18, 17, 18, 19, 19, 18, 19, 18, 17, 16, 17, 14, 13],
            [4, 5, 12, 13, 15, 15, 15, 19, 20, 19, 18, 10, 10, 17, 19, 20, 19, 14, 14, 15, 12, 12, 5, 4],
            [2, 30, 30, 30, 31, 31, 31, 19, 17, 13, 14, 14, 15, 16, 19, 25, 24, 24, 23, 21, 16, 9, 7, 6],
            [26, 31, 32, 31, 29, 8, 8, 8, 8, 7, 8, 8, 10, 12, 13, 14, 15, 14, 21, 21, 19, 13, 12, 1],
            [31, 31, 31, 31, 32, 32, 15, 14, 12, 12, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 9, 9, 9, 7],
            [28, 30, 32, 32, 32, 26, 22, 10, 9, 9, 9, 9, 8, 8, 7, 7, 7, 9, 10, 9, 7, 4, 4, 3],
            [14, 16, 20, 22, 24, 15, 15, 16, 15, 13, 10, 8, 9, 10, 9, 8, 9, 9, 9, 18, 19, 20, 18, 16],
            [19, 32, 32, 32, 20, 6, 5, 5, 4, 4, 4, 4, 4, 5, 5, 5, 6, 4, 5, 27, 31, 31, 30, 23],
            [31, 32, 32, 32, 32, 8, 7, 9, 9, 9, 7, 8, 7, 7, 10, 10, 9, 7, 8, 32, 32, 32, 32, 31],
            [1, 9, 11, 17, 20, 12, 12, 11, 10, 8, 7, 7, 7, 7, 12, 16, 17, 16, 23, 22, 19, 16, 2, 1],
            [30, 30, 30, 30, 30, 11, 11, 9, 10, 11, 12, 14, 18, 21, 20, 19, 17, 14, 15, 5, 5, 4, 4, 2],
            [3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 32, 32, 32, 32, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4],
            [2, 2, 7, 8, 9, 11, 11, 8, 8, 8, 19, 19, 19, 19, 8, 8, 7, 11, 10, 8, 8, 7, 3, 2]])

    Y = np.array([
        [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0],

        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]])      # 期望输出

    nn.fit(X, Y)                    # 训练网络

    #print('w:', nn.weights)          # 调整后的权值列表
    
    for s in X:
        print(s, nn.predict(s))     # 测试
